{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57670716-b7b9-4e16-837a-88ff5322e543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy: 1.26.4\n",
      "Pandas: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "print(\"NumPy:\", numpy.__version__)\n",
    "print(\"Pandas:\", pandas.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1382440c-4923-49c5-bf38-2164b25e486d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\anaconda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– RAG Chatbot Ready. Ask about loan approvals (type 'exit' to quit)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What factors affect loan approval?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Male | Female | 3+ | Not Graduate | No | 3522 | 0.0 | 81.0 | 180.0 | 1.0 | Rural | N | LP001207 | Male | Yes | 0 | Not Graduate | Yes | 2609 | 3449.0 | 165.0 | 180.0 | 0.0 | Rural | N\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  history\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: LP002772 is a list of women's and men's records from the 18th and 19th centuries. LP001917 is a list of women's and men's records from the 18th and 19th centuries. LP001917 is a list of women's and men's records from the 18th and 19th centuries. LP001917 is a list of women's and men's records from the 18th and 19th centuries. LP001917 is a list of women's and men's records from the 18th and 19th centuries. LP001917 is a list of women's and men's records from the 18th and 19th centuries. LP001917 is a list of women's and men's records from the 18th and 19th centuries. LP001917 is a list of women's and men's records from the 18th and 19th centuries. LP001917 is a list of women's and men's records from the 18th and 19th centuries. LP001917 is a list of women's\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "\n",
    "# Step 1: Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\rcish\\Downloads\\loan_data.csv\\Training Dataset.csv\")  # Make sure the file exists in your working directory\n",
    "df = df.dropna().astype(str)\n",
    "documents = df.apply(lambda row: \" | \".join(row), axis=1).tolist()\n",
    "\n",
    "# Step 2: Embed documents\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "doc_embeddings = embedder.encode(documents, convert_to_numpy=True)\n",
    "\n",
    "# Step 3: Create FAISS index\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# Step 4: Load generation model\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "\n",
    "# Step 5: Function to answer questions\n",
    "def answer_question(question, k=3):\n",
    "    query_embedding = embedder.encode([question])\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    context = \"\\n\".join([documents[i] for i in indices[0]])\n",
    "\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    output = generator(prompt, max_length=200, do_sample=True)[0]['generated_text']\n",
    "    return output\n",
    "\n",
    "# Step 6: Chat loop\n",
    "print(\"ðŸ¤– RAG Chatbot Ready. Ask about loan approvals (type 'exit' to quit)\\n\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    print(\"Bot:\", answer_question(user_input))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
